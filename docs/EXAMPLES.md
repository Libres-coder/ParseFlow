# ParseFlow ä½¿ç”¨ç¤ºä¾‹

## ğŸ“š ç›®å½•

- [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
- [åœ¨ Windsurf ä¸­ä½¿ç”¨](#åœ¨-windsurf-ä¸­ä½¿ç”¨)
- [ä½œä¸ºåº“ä½¿ç”¨](#ä½œä¸ºåº“ä½¿ç”¨)
- [é«˜çº§åœºæ™¯](#é«˜çº§åœºæ™¯)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

---

## åŸºç¡€ä½¿ç”¨

### ç¤ºä¾‹ 1ï¼šå¿«é€ŸæŸ¥çœ‹ PDF ä¿¡æ¯

```javascript
import { PDFParser } from '@parseflow/core';

const parser = new PDFParser();
const metadata = await parser.getMetadata('document.pdf');

console.log(`æ ‡é¢˜: ${metadata.info.Title}`);
console.log(`é¡µæ•°: ${metadata.metadata.pageCount}`);
console.log(`ä½œè€…: ${metadata.info.Author}`);
```

### ç¤ºä¾‹ 2ï¼šæå–å…¨éƒ¨æ–‡æœ¬

```javascript
const parser = new PDFParser();
const text = await parser.extractText('document.pdf');
console.log(text);
```

### ç¤ºä¾‹ 3ï¼šæå–ç‰¹å®šé¡µé¢

```javascript
// æå–ç¬¬ 5 é¡µ
const page5 = await parser.extractPage('document.pdf', 5);

// æå–ç¬¬ 10-20 é¡µ
const range = await parser.extractRange('document.pdf', 10, 20);
```

### ç¤ºä¾‹ 4ï¼šæœç´¢å…³é”®è¯

```javascript
const results = await parser.search('document.pdf', 'é‡è¦é€šçŸ¥', {
  caseSensitive: false,
  maxResults: 10,
});

results.forEach((r) => {
  console.log(`ç¬¬ ${r.page} é¡µ: ${r.context}`);
});
```

---

## åœ¨ Windsurf ä¸­ä½¿ç”¨

### åœºæ™¯ 1ï¼šæ–‡æ¡£å®¡é˜…

**ä½ çš„é—®é¢˜ï¼š**
```
è¯·å¸®æˆ‘å®¡é˜… D:\contracts\agreement.pdfï¼Œé‡ç‚¹å…³æ³¨ä»˜æ¬¾æ¡æ¬¾
```

**Windsurf ä¼šï¼š**
1. è°ƒç”¨ ParseFlow æå–å…¨æ–‡
2. ä½¿ç”¨ AI åˆ†æå†…å®¹
3. çªå‡ºæ˜¾ç¤ºä»˜æ¬¾ç›¸å…³æ¡æ¬¾

### åœºæ™¯ 2ï¼šå¿«é€ŸæŸ¥æ‰¾

**ä½ çš„é—®é¢˜ï¼š**
```
åœ¨ D:\manuals\ æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ PDF ä¸­æ‰¾åˆ°æåˆ°"API å¯†é’¥"çš„åœ°æ–¹
```

**Windsurf ä¼šï¼š**
1. éå†æ–‡ä»¶å¤¹
2. å¯¹æ¯ä¸ª PDF è°ƒç”¨æœç´¢
3. æ±‡æ€»ç»“æœ

### åœºæ™¯ 3ï¼šæ–‡æ¡£å¯¹æ¯”

**ä½ çš„é—®é¢˜ï¼š**
```
å¯¹æ¯” D:\v1.pdf å’Œ D:\v2.pdf çš„å·®å¼‚
```

**Windsurf ä¼šï¼š**
1. æå–ä¸¤ä¸ªæ–‡æ¡£çš„æ–‡æœ¬
2. è¿›è¡Œå·®å¼‚åˆ†æ
3. æ€»ç»“ä¸»è¦å˜åŒ–

### åœºæ™¯ 4ï¼šå†…å®¹æ€»ç»“

**ä½ çš„é—®é¢˜ï¼š**
```
æ€»ç»“ D:\report.pdf çš„æ ¸å¿ƒè§‚ç‚¹ï¼Œç”¨ 5 ä¸ªè¦ç‚¹åˆ—å‡º
```

**Windsurf ä¼šï¼š**
1. æå–å…¨æ–‡
2. AI åˆ†æå’Œæ€»ç»“
3. æ ¼å¼åŒ–è¾“å‡º

---

## ä½œä¸ºåº“ä½¿ç”¨

### ç¤ºä¾‹ 5ï¼šåœ¨ Express åº”ç”¨ä¸­ä½¿ç”¨

```javascript
import express from 'express';
import { PDFParser } from '@parseflow/core';
import multer from 'multer';

const app = express();
const upload = multer({ dest: 'uploads/' });
const parser = new PDFParser();

// ä¸Šä¼ å¹¶è§£æ PDF
app.post('/api/pdf/upload', upload.single('pdf'), async (req, res) => {
  try {
    const metadata = await parser.getMetadata(req.file.path);
    const text = await parser.extractText(req.file.path);

    res.json({
      success: true,
      metadata,
      textLength: text.length,
      preview: text.substring(0, 200),
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// æœç´¢ PDF
app.get('/api/pdf/search', async (req, res) => {
  const { file, query } = req.query;
  const results = await parser.search(file, query);
  res.json(results);
});

app.listen(3000);
```

### ç¤ºä¾‹ 6ï¼šæ‰¹é‡å¤„ç†

```javascript
import { PDFParser } from '@parseflow/core';
import { readdir } from 'fs/promises';
import { join } from 'path';

async function processBatch(directory) {
  const parser = new PDFParser();
  const files = await readdir(directory);
  const pdfFiles = files.filter((f) => f.endsWith('.pdf'));

  const results = [];

  for (const file of pdfFiles) {
    const filePath = join(directory, file);
    try {
      const metadata = await parser.getMetadata(filePath);
      const text = await parser.extractText(filePath);

      results.push({
        file,
        pageCount: metadata.metadata.pageCount,
        textLength: text.length,
        author: metadata.info.Author,
      });

      console.log(`âœ… å¤„ç†å®Œæˆ: ${file}`);
    } catch (error) {
      console.error(`âŒ å¤„ç†å¤±è´¥: ${file}`, error.message);
    }
  }

  return results;
}

// ä½¿ç”¨
const results = await processBatch('D:\\documents');
console.log(`å…±å¤„ç† ${results.length} ä¸ªæ–‡ä»¶`);
```

### ç¤ºä¾‹ 7ï¼šè‡ªå®šä¹‰ç¼“å­˜

```javascript
import { PDFParser } from '@parseflow/core';

const parser = new PDFParser({
  cache: {
    enabled: true,
    maxSize: 50, // ç¼“å­˜ 50 ä¸ªæ–‡ä»¶
    maxAge: 3600000, // 1 å°æ—¶è¿‡æœŸ
    directory: './my-cache',
  },
});

// ç¬¬ä¸€æ¬¡è°ƒç”¨ - è§£æå¹¶ç¼“å­˜
const text1 = await parser.extractText('large.pdf');

// ç¬¬äºŒæ¬¡è°ƒç”¨ - ä»ç¼“å­˜è¯»å–ï¼ˆå¿«é€Ÿï¼‰
const text2 = await parser.extractText('large.pdf');
```

---

## é«˜çº§åœºæ™¯

### ç¤ºä¾‹ 8ï¼šç»“åˆ LangChain å®ç° RAG

```javascript
import { PDFParser } from '@parseflow/core';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { MemoryVectorStore } from 'langchain/vectorstores/memory';
import { OpenAIEmbeddings } from 'langchain/embeddings/openai';

async function createPDFVectorStore(pdfPath) {
  // 1. æå– PDF æ–‡æœ¬
  const parser = new PDFParser();
  const text = await parser.extractText(pdfPath);

  // 2. åˆ†å‰²æ–‡æœ¬
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  const docs = await splitter.createDocuments([text]);

  // 3. åˆ›å»ºå‘é‡å­˜å‚¨
  const vectorStore = await MemoryVectorStore.fromDocuments(docs, new OpenAIEmbeddings());

  return vectorStore;
}

// ä½¿ç”¨
const store = await createPDFVectorStore('knowledge.pdf');
const results = await store.similaritySearch('å¦‚ä½•ä½¿ç”¨ APIï¼Ÿ', 3);
```

### ç¤ºä¾‹ 9ï¼šPDF å†…å®¹ç›‘æ§

```javascript
import { PDFParser } from '@parseflow/core';
import { watch } from 'chokidar';

const parser = new PDFParser();

// ç›‘æ§ç›®å½•ä¸­çš„ PDF å˜åŒ–
const watcher = watch('D:\\documents\\*.pdf', {
  persistent: true,
});

watcher.on('add', async (path) => {
  console.log(`æ–°æ–‡ä»¶: ${path}`);
  const metadata = await parser.getMetadata(path);
  console.log(`é¡µæ•°: ${metadata.metadata.pageCount}`);

  // æ£€æŸ¥å…³é”®è¯
  const hasKeyword = await checkKeyword(path, 'ç´§æ€¥');
  if (hasKeyword) {
    console.log('âš ï¸ å‘ç°åŒ…å«"ç´§æ€¥"çš„æ–‡æ¡£ï¼');
    // å‘é€é€šçŸ¥...
  }
});

async function checkKeyword(path, keyword) {
  const results = await parser.search(path, keyword);
  return results.length > 0;
}
```

### ç¤ºä¾‹ 10ï¼šPDF åˆ° Markdown è½¬æ¢

```javascript
import { PDFParser } from '@parseflow/core';
import { writeFile } from 'fs/promises';

async function pdfToMarkdown(pdfPath, mdPath) {
  const parser = new PDFParser();

  // è·å–å…ƒæ•°æ®
  const metadata = await parser.getMetadata(pdfPath);

  // æå–æ–‡æœ¬ï¼ˆæ ¼å¼åŒ–ï¼‰
  const text = await parser.extractText(pdfPath, { strategy: 'formatted' });

  // æ„å»º Markdown
  const markdown = `# ${metadata.info.Title || 'æœªå‘½åæ–‡æ¡£'}

**ä½œè€…**: ${metadata.info.Author || 'N/A'}
**åˆ›å»ºæ—¥æœŸ**: ${metadata.info.CreationDate || 'N/A'}
**é¡µæ•°**: ${metadata.metadata.pageCount}

---

${text}

---

*ç”± ParseFlow è‡ªåŠ¨ç”Ÿæˆ*
`;

  await writeFile(mdPath, markdown, 'utf-8');
  console.log(`âœ… å·²è½¬æ¢: ${mdPath}`);
}

// ä½¿ç”¨
await pdfToMarkdown('report.pdf', 'report.md');
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šåˆåŒç®¡ç†ç³»ç»Ÿ

```javascript
// è‡ªåŠ¨æå–åˆåŒå…³é”®ä¿¡æ¯
async function extractContractInfo(pdfPath) {
  const parser = new PDFParser();
  const text = await parser.extractText(pdfPath);

  // ä½¿ç”¨æ­£åˆ™æå–å…³é”®ä¿¡æ¯
  const partyA = text.match(/ç”²æ–¹[ï¼š:]\s*([^\n]+)/)?.[1];
  const partyB = text.match(/ä¹™æ–¹[ï¼š:]\s*([^\n]+)/)?.[1];
  const amount = text.match(/åˆåŒé‡‘é¢[ï¼š:]\s*([^\n]+)/)?.[1];
  const date = text.match(/ç­¾è®¢æ—¥æœŸ[ï¼š:]\s*([^\n]+)/)?.[1];

  return { partyA, partyB, amount, date };
}
```

### æ¡ˆä¾‹ 2ï¼šå­¦æœ¯è®ºæ–‡åˆ†æ

```javascript
// æå–è®ºæ–‡æ‘˜è¦å’Œå…³é”®è¯
async function analyzePaper(pdfPath) {
  const parser = new PDFParser();
  const page1 = await parser.extractPage(pdfPath, 1);

  const abstract = page1.match(/Abstract(.+?)Keywords/s)?.[1]?.trim();
  const keywords = page1.match(/Keywords[ï¼š:]\s*([^\n]+)/)?.[1];

  const metadata = await parser.getMetadata(pdfPath);

  return {
    title: metadata.info.Title,
    author: metadata.info.Author,
    abstract,
    keywords: keywords?.split(/[,ï¼Œ;ï¼›]/).map((k) => k.trim()),
  };
}
```

### æ¡ˆä¾‹ 3ï¼šå‘ç¥¨æ‰¹é‡å¤„ç†

```javascript
// æ‰¹é‡æå–å‘ç¥¨ä¿¡æ¯
async function processInvoices(directory) {
  const parser = new PDFParser();
  const files = await readdir(directory);
  const invoices = [];

  for (const file of files.filter((f) => f.endsWith('.pdf'))) {
    const path = join(directory, file);
    const text = await parser.extractText(path);

    // æå–å‘ç¥¨å·ã€é‡‘é¢ç­‰
    const invoiceNo = text.match(/å‘ç¥¨å·ç [ï¼š:]\s*(\d+)/)?.[1];
    const amount = text.match(/ä»·ç¨åˆè®¡[ï¼š:]\s*Â¥?([\d,.]+)/)?.[1];
    const date = text.match(/å¼€ç¥¨æ—¥æœŸ[ï¼š:]\s*([\d-/]+)/)?.[1];

    invoices.push({ file, invoiceNo, amount, date });
  }

  return invoices;
}
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```javascript
try {
  const text = await parser.extractText(path);
} catch (error) {
  if (error.code === 'ENOENT') {
    console.error('æ–‡ä»¶ä¸å­˜åœ¨');
  } else if (error.message.includes('Invalid PDF')) {
    console.error('PDF æ–‡ä»¶æŸå');
  } else {
    console.error('æœªçŸ¥é”™è¯¯:', error);
  }
}
```

### 2. æ€§èƒ½ä¼˜åŒ–

```javascript
// âœ… å¥½ï¼šåªæå–éœ€è¦çš„é¡µé¢
const page1 = await parser.extractPage(path, 1);

// âŒ å·®ï¼šæå–å…¨éƒ¨ç„¶ååªç”¨ç¬¬ä¸€é¡µ
const allText = await parser.extractText(path);
const page1Text = allText.split('\n\n\n')[0];
```

### 3. èµ„æºç®¡ç†

```javascript
// å¤„ç†å¤§é‡æ–‡ä»¶æ—¶ï¼Œåˆ›å»ºå¤šä¸ª parser å®ä¾‹
const parsers = Array.from({ length: 5 }, () => new PDFParser());

async function processWithPool(files) {
  let index = 0;
  for (const file of files) {
    const parser = parsers[index % parsers.length];
    await parser.extractText(file);
    index++;
  }
}
```

---

## ğŸ”— æ›´å¤šèµ„æº

- [API å®Œæ•´æ–‡æ¡£](./API.md)
- [æ¶æ„è®¾è®¡](./ARCHITECTURE.md)
- [å¸¸è§é—®é¢˜](./FAQ.md)

---

**æœ€åæ›´æ–°**: 2024-11-26
